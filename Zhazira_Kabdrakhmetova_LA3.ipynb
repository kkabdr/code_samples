{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tU8GUy_UE49TE1x5-rbZ9msTBOhUhITs","timestamp":1694768915012}],"gpuType":"T4","collapsed_sections":["HYuu-PmqiP86"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Installations"],"metadata":{"id":"HYuu-PmqiP86"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmFf7jza1ixU","outputId":"13555cf8-d1fb-4967-8626-cd9e95e42ad9","executionInfo":{"status":"ok","timestamp":1695633132001,"user_tz":-360,"elapsed":14,"user":{"displayName":"Zhazira Kabdrakhmetova","userId":"11276664224039616990"}}},"source":["#!apt-get update\n","#!apt-get install cuda-10.1\n","!nvcc --version"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2022 NVIDIA Corporation\n","Built on Wed_Sep_21_10:33:58_PDT_2022\n","Cuda compilation tools, release 11.8, V11.8.89\n","Build cuda_11.8.r11.8/compiler.31833905_0\n"]}]},{"cell_type":"code","metadata":{"id":"IdWxYqPoAald","colab":{"base_uri":"https://localhost:8080/"},"outputId":"635923b8-f90e-4ab1-dfd5-53aba8914523","executionInfo":{"status":"ok","timestamp":1695633139394,"user_tz":-360,"elapsed":7403,"user":{"displayName":"Zhazira Kabdrakhmetova","userId":"11276664224039616990"}}},"source":["#!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n","!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n","%load_ext nvcc_plugin"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n","  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-810qg1i0\n","  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-810qg1i0\n","  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: NVCCPlugin\n","  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=9a29b7b10d20f6c7174d70076474275e9d8e0d2272e50e8dbe66e970508bfbc4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3y4_5r44/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n","Successfully built NVCCPlugin\n","Installing collected packages: NVCCPlugin\n","Successfully installed NVCCPlugin-0.0.2\n","created output directory at /content/src\n","Out bin /content/result.out\n"]}]},{"cell_type":"markdown","source":["# Lab 3"],"metadata":{"id":"b8-IQ9vCuum0"}},{"cell_type":"code","source":["lab3 = \"\"\"\n","// Zhazira Kabdrakhmetova || ID: 201980168\n","// Course: CSCI325 Fall23 || LA3\n","\n","#define N (512*512)\n","// My system wasn't able to run code with 1024x1024, so I decreased it\n","// it was due to my hardware or google colab limitations\n","#define K 4\n","#define K2 32\n","#define K3 512\n","#define K4 256\n","#include <stdio.h>\n","#include <time.h>\n","\n","// CONCLUSION: WORST CONFIG WAS 1 BLOCK USAGE\n","// BEST CONFIG WAS USING 512 THREADS PER BLOCK, 512 BLOCKS\n","// WITH INCREASING NUMBER OF THREADS PER BLOCK RESULTS WERE IMPROVED\n","\n","// ALSO WITH MORE THREADS IN A SINGLE BLOCK PERFORMANCE IMPROVES\n","// MAYBE BECAUSE IF THERE ARE TOO MANY BLOCKS WE WILL HAVE A LOT OF\n","// ATOMIC FUNCTIONS AND BLOCK COORDINATION\n","// IT'S BETTER TO SPREAD THE TASKS EVENLY\n","\n","// BUT HAVING JUST ONE BLOCK WITH SAME NUMBER OF THREADS(512)\n","// IS NOT EFFICIENT - BAD MEMORY UTILIZATION, SO SLOWER EXEC TIME\n","\n","void dpSequential(float A[N], float B[N], float *c) {\n","    for (int i = 0; i < N; i++) {\n","        *c = *c + A[i] * B[i];\n","    }\n","}\n","\n","__global__ void dpGlobal(float A[N], float B[N], float *c){\n","    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int threadNum = blockDim.x * gridDim.x;\n","    float new_c = 0;\n","\n","    for (int i = threadIndex; i < N; i += threadNum) {\n","        new_c = new_c + A[i] * B[i];\n","    }\n","\n","    atomicAdd(c, new_c);\n","}\n","\n","__global__ void dpShared_Conf1(float A[N], float B[N], float *c){\n","    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int threadNum = blockDim.x * gridDim.x;\n","    float new_c = 0;\n","\n","    for (int i = threadIndex; i < N; i += threadNum) {\n","        new_c = new_c + A[i] * B[i];\n","    }\n","\n","    // start a variable to store all c values accross threads\n","    __shared__ float new_cs[K];\n","    new_cs[threadIndex] = new_c;\n","    __syncthreads();\n","\n","    // parallel reduction to find a sum of separate c values accross threads\n","    // after the iteration decrease the scope by two\n","    // continually add array elements until the last c values is new_cs[0]\n","    for (int i = threadNum / 2; i > 0; i /= 2) {\n","        if (threadIndex < i) {\n","            new_cs[threadIndex] += new_cs[threadIndex + i];\n","        }\n","    }\n","     __syncthreads();\n","\n","    // make this equality only once\n","    if (threadIndex == 0) {\n","        *c = new_cs[0];\n","    }\n","}\n","\n","__global__ void dpShared_ConfMany(float A[N], float B[N], float *new_cs){\n","    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int threadNum = blockDim.x * gridDim.x;\n","    float new_c = 0;\n","\n","    for (int i = threadIndex; i < N; i += threadNum) {\n","        new_c = new_c + A[i] * B[i];\n","    }\n","\n","    __shared__ float new_block_cs[K];\n","    new_block_cs[threadIdx.x] = new_c;\n","    __syncthreads();\n","\n","    for (int i = blockDim.x / 2; i > 0; i /= 2) {\n","        if (threadIdx.x < i) {\n","            new_block_cs[threadIdx.x] += new_block_cs[threadIdx.x + i];\n","        }\n","        __syncthreads();\n","    }\n","    if (threadIdx.x == 0) {\n","        new_cs[blockIdx.x] = new_block_cs[0];\n","    }\n","}\n","\n","__global__ void dpShared_ConfMany2(float A[N], float B[N], float *new_cs){\n","    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int threadNum = blockDim.x * gridDim.x;\n","    float new_c = 0;\n","\n","    for (int i = threadIndex; i < N; i += threadNum) {\n","        new_c = new_c + A[i] * B[i];\n","    }\n","\n","    __shared__ float new_block_cs[K2];\n","    new_block_cs[threadIdx.x] = new_c;\n","    __syncthreads();\n","\n","    for (int i = blockDim.x / 2; i > 0; i /= 2) {\n","        if (threadIdx.x < i) {\n","            new_block_cs[threadIdx.x] += new_block_cs[threadIdx.x + i];\n","        }\n","        __syncthreads();\n","    }\n","    if (threadIdx.x == 0) {\n","        new_cs[blockIdx.x] = new_block_cs[0];\n","    }\n","}\n","\n","__global__ void dpShared_ConfMany3(float A[N], float B[N], float *new_cs){\n","    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int threadNum = blockDim.x * gridDim.x;\n","    float new_c = 0;\n","\n","    for (int i = threadIndex; i < N; i += threadNum) {\n","        new_c = new_c + A[i] * B[i];\n","    }\n","\n","    __shared__ float new_block_cs[K3];\n","    new_block_cs[threadIdx.x] = new_c;\n","    __syncthreads();\n","\n","    for (int i = blockDim.x / 2; i > 0; i /= 2) {\n","        if (threadIdx.x < i) {\n","            new_block_cs[threadIdx.x] += new_block_cs[threadIdx.x + i];\n","        }\n","        __syncthreads();\n","    }\n","    if (threadIdx.x == 0) {\n","        new_cs[blockIdx.x] = new_block_cs[0];\n","    }\n","}\n","\n","__global__ void dpShared_ConfMany4(float A[N], float B[N], float *new_cs){\n","    int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n","    int threadNum = blockDim.x * gridDim.x;\n","    float new_c = 0;\n","\n","    for (int i = threadIndex; i < N; i += threadNum) {\n","        new_c = new_c + A[i] * B[i];\n","    }\n","\n","    __shared__ float new_block_cs[K4];\n","    new_block_cs[threadIdx.x] = new_c;\n","    __syncthreads();\n","\n","    for (int i = blockDim.x / 2; i > 0; i /= 2) {\n","        if (threadIdx.x < i) {\n","            new_block_cs[threadIdx.x] += new_block_cs[threadIdx.x + i];\n","        }\n","        __syncthreads();\n","    }\n","    if (threadIdx.x == 0) {\n","        new_cs[blockIdx.x] = new_block_cs[0];\n","    }\n","}\n","\n","// TO DO: create more kernel function names.\n","\n","int main(){\n","    clock_t start_time, end_time;\n","\n","    float A[N], B[N];\n","    float c=0;\n","\n","    for (int i = 0; i < N; i++){\n","        A[i] = 1.0f;\n","        B[i] = 2.0f;\n","    }\n","\n","    start_time = clock();\n","    // FIRST: SEQUENTIAL\n","    dpSequential(A, B, &c);\n","    printf(\"result of sequential = %.2f\\\\n\", c);\n","    end_time = clock();\n","    double elapsed_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n","    printf(\"Elapsed time for config 1: %f \\\\n\", elapsed_time);\n","\n","\n","\n","\n","    // CUDA MEMORY MANAGEMENT\n","    float *deviceA, *deviceB, *deviceC;\n","    cudaMalloc((void **)&deviceA, N * sizeof(float));\n","    cudaMalloc((void **)&deviceB, N * sizeof(float));\n","    cudaMalloc((void **)&deviceC, sizeof(float));\n","\n","    cudaMemcpy(deviceA, A, N * sizeof(float), cudaMemcpyHostToDevice);\n","    cudaMemcpy(deviceB, B, N * sizeof(float), cudaMemcpyHostToDevice);\n","\n","\n","    start_time = clock();\n","    // SECOND: GLOBAL MEMORY\n","    c=0;\n","    cudaMemcpy(deviceC, &c, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    dpGlobal<<<(N + K - 1) / K, K>>>(deviceA, deviceB, deviceC);\n","    cudaMemcpy(&c, deviceC, sizeof(float), cudaMemcpyDeviceToHost);\n","    printf(\"result of Global memory = %.2f\\\\n\", c);\n","    end_time = clock();\n","    elapsed_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n","    printf(\"Elapsed time for config 2: %f \\\\n\", elapsed_time);\n","\n","\n","    start_time = clock();\n","    // THIRD: SHARED MEMORY - 1 BLOCK ONLY\n","    c=0;\n","    cudaMemcpy(deviceC, &c, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    dim3 threadsPerBlock(32,32);\n","    dpShared_Conf1<<<1, threadsPerBlock>>>(deviceA, deviceB, deviceC);\n","    cudaMemcpy(&c, deviceC, sizeof(float), cudaMemcpyDeviceToHost);\n","    printf(\"result of Shared memory (1 block) = %.2f\\\\n\", c);\n","    end_time = clock();\n","     elapsed_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n","    printf(\"Elapsed time for config 3: %f \\\\n\", elapsed_time);\n","\n","\n","    start_time = clock();\n","    // FOURTH: SHARED MEMORY - MULTIPLE BLOCKS\n","    c=0;\n","    cudaMemcpy(deviceC, &c, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    int numBlocks = (N + K - 1) / K;\n","    dpShared_ConfMany<<<numBlocks, K>>>(deviceA, deviceB, deviceC);\n","    cudaMemcpy(&c, deviceC, sizeof(float), cudaMemcpyDeviceToHost);\n","    float result = 0.0f;\n","\n","    for (int i = 0; i < numBlocks; i++) {\n","        result += c;\n","    }\n","    printf(\"result of Shared memory (multiple blocks K=4) = %.2f\\\\n\", result);\n","    end_time = clock();\n","     elapsed_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n","    printf(\"Elapsed time for config 4: %f \\\\n\", elapsed_time);\n","\n","\n","    start_time = clock();\n","    // FIFTH: SHARED MEMORY - MULTIPLE BLOCKS K = 32\n","    c=0;\n","    cudaMemcpy(deviceC, &c, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    numBlocks = (N + K2 - 1) / K2;\n","    dpShared_ConfMany2<<<numBlocks, K2>>>(deviceA, deviceB, deviceC);\n","    cudaMemcpy(&c, deviceC, sizeof(float), cudaMemcpyDeviceToHost);\n","    result = 0.0f;\n","\n","    for (int i = 0; i < numBlocks; i++) {\n","        result += c;\n","    }\n","    printf(\"result of Shared memory (multiple blocks K=32) = %.2f\\\\n\", result);\n","    end_time = clock();\n","     elapsed_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n","    printf(\"Elapsed time for config 5: %f \\\\n\", elapsed_time);\n","\n","\n","    start_time = clock();\n","    // SIXTH: SHARED MEMORY - MULTIPLE BLOCKS K = 256\n","    c=0;\n","    cudaMemcpy(deviceC, &c, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    numBlocks = (N + K4 - 1) / K4;\n","    dpShared_ConfMany3<<<numBlocks, K4>>>(deviceA, deviceB, deviceC);\n","    cudaMemcpy(&c, deviceC, sizeof(float), cudaMemcpyDeviceToHost);\n","    result = 0.0f;\n","\n","    for (int i = 0; i < numBlocks; i++) {\n","        result += c;\n","    }\n","    printf(\"result of Shared memory (multiple blocks K=256) = %.2f\\\\n\", result);\n","    end_time = clock();\n","     elapsed_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n","    printf(\"Elapsed time for config 6: %f \\\\n\", elapsed_time);\n","\n","\n","    start_time = clock();\n","    // SEVENTH: SHARED MEMORY - MULTIPLE BLOCKS K = 512\n","    c=0;\n","    cudaMemcpy(deviceC, &c, sizeof(float), cudaMemcpyHostToDevice);\n","\n","    numBlocks = (N + K3 - 1) / K3;\n","    dpShared_ConfMany3<<<numBlocks, K3>>>(deviceA, deviceB, deviceC);\n","    cudaMemcpy(&c, deviceC, sizeof(float), cudaMemcpyDeviceToHost);\n","    result = 0.0f;\n","\n","    for (int i = 0; i < numBlocks; i++) {\n","        result += c;\n","    }\n","    printf(\"result of Shared memory (multiple blocks K=512) = %.2f\\\\n\", result);\n","    end_time = clock();\n","        elapsed_time = ((double)(end_time - start_time)) / CLOCKS_PER_SEC;\n","    printf(\"Elapsed time for config 7: %f \\\\n\", elapsed_time);\n","\n","    cudaFree(deviceA);\n","    cudaFree(deviceB);\n","    cudaFree(deviceC);\n","\n","    return 0;\n","\n","\n","  // TO DO\n","  //1. Invoke the kernel with CUDA memory management\n","  //2. Verify the result of the scholar value (MUST be 2*N = 2*1024*1024 = 2097152)\n","}\n","\n","\"\"\""],"metadata":{"id":"e9i7wjhwGwnz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_file = open(\"Zhazira_Kabdrakhmetova_LA3.cu\", \"w\")\n","text_file.write(lab3)\n","text_file.close()\n","!nvcc -o Zhazira_Kabdrakhmetova_LA3 Zhazira_Kabdrakhmetova_LA3.cu -Xcompiler -fopenmp -lgomp\n","!./Zhazira_Kabdrakhmetova_LA3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYxY0YQpI_7r","executionInfo":{"status":"ok","timestamp":1695633196519,"user_tz":-360,"elapsed":3626,"user":{"displayName":"Zhazira Kabdrakhmetova","userId":"11276664224039616990"}},"outputId":"9abcf3cf-6a29-4373-aef6-3e543359bdf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["result of sequential = 524288.00\n","Elapsed time for config 1: 0.001051 \n","result of Global memory = 524288.00\n","Elapsed time for config 2: 0.001081 \n","result of Shared memory (1 block) = 3751936.00\n","Elapsed time for config 3: 0.002136 \n","result of Shared memory (multiple blocks K=4) = 524288.00\n","Elapsed time for config 4: 0.000543 \n","result of Shared memory (multiple blocks K=32) = 524288.00\n","Elapsed time for config 5: 0.000106 \n","result of Shared memory (multiple blocks K=256) = 524288.00\n","Elapsed time for config 6: 0.000065 \n","result of Shared memory (multiple blocks K=512) = 524288.00\n","Elapsed time for config 7: 0.000065 \n"]}]}]}